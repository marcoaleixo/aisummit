{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AISummit - machine_translation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/marcoaleixo/aisummit/blob/master/AISummit%20-%20machine_translation.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "ZyO_fU_lNRpv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Seq2Seq para tradução inglês - Português"
      ]
    },
    {
      "metadata": {
        "id": "CT46WBMiNRpy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Modelo seq2seq simples para tradução de texto de inglês para português com leitura character por character."
      ]
    },
    {
      "metadata": {
        "id": "4ktCEqXFOccB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#executar apenas uma vez\n",
        "!pip3 install --upgrade torch torchvision googledrivedownloader\n",
        "\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "gdd.download_file_from_google_drive(file_id='1_NiGFhSifCeu5_EEAbCIAh9rzSl1kXPG', dest_path='./por.txt', unzip=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KqlBfpBfNRpy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, CuDNNLSTM, Dense, LSTM\n",
        "import numpy as np\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qt_VuDhfgh0P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_path = 'por.txt' #Arquivo com pares inglês-português\n",
        "latent_dim = 264  #Tamanho do estado h\n",
        "num_samples = 10000  # Numero de amostras a serem usadas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bTn2MlOigrqQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Aqui vamos carregar, fazer o um pré processamento e visualização do nosso dataset "
      ]
    },
    {
      "metadata": {
        "id": "IQBdvyvMNRp3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "input_texts = []\n",
        "target_texts = []\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "\n",
        "lines = open(data_path, encoding=\"utf8\").read().split('\\n')\n",
        "\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    input_text, target_text = line.split('\\t')\n",
        "    # Usaremos um \\t para indicar o inicio de uma frase e \\n para indicar o fim\n",
        "    target_text = '\\t' + target_text + '\\n'\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    for char in input_text:\n",
        "        if char not in input_characters:\n",
        "            input_characters.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in target_characters:\n",
        "            target_characters.add(char)\n",
        "\n",
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "print('Número de amostras:', len(input_texts))\n",
        "print('Número de tokens unicos de input:', num_encoder_tokens)\n",
        "print('Number de tokens unicos de output:', num_decoder_tokens)\n",
        "print('Tamanho máximo de input:', max_encoder_seq_length)\n",
        "print('Tamanho máximo de output:', max_decoder_seq_length)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GMYBEJvKg-3B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Aqui vamos criar o dicionário de tokens para podermos fazer a conversão para texto futuramente"
      ]
    },
    {
      "metadata": {
        "id": "HgwJBaaug1XN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "input_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(target_characters)])\n",
        "\n",
        "print()\n",
        "print('input_char: ', input_characters)\n",
        "print()\n",
        "print('input_token_index: ', input_token_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4zQSvoIlNRp6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#cria arrays de 0 para fazer o padding\n",
        "encoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')\n",
        "\n",
        "#preenche os arrays com os caracteres\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
        "    for t, char in enumerate(target_text):\n",
        "        # decoder_target_data é a saída esperada do decoder, então deve estar um tempo a frente e não incluir o \\t\n",
        "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
        "        if t > 0:\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
        "\n",
        "#one hot encoding\n",
        "print(input_texts[0])\n",
        "for i in encoder_input_data[0]:\n",
        "    print(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rjxbzDznNRp-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Seq2Seq"
      ]
    },
    {
      "metadata": {
        "id": "y0WWtobYNRp-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Aqui criamos um modelo Seq2Seq simples com uma camada de LSTM como encoder e outra como decoder.\n",
        "\n",
        "O encoder retorna seus estados que serão a inicialização do decoder\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "I-OJcwonNRp_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Teacher Forcing:\n",
        "\n",
        "Suponha a frase inicial:\n",
        "Mary had a little lamb whose fleece was white as snow\n",
        "\n",
        "A mesma frase com os marcadores de inicio e fim:\n",
        "[START] Mary had a little lamb whose fleece was white as snow [END]\n",
        "\n",
        "Treinamento:\n",
        "\n",
        "X                        y\n",
        "--------------------------\n",
        "[START],                 ?\n",
        "[START], Mary,           ?\n",
        "[START], Mary, had,      ?\n",
        "[START], Mary, had, a,   ?\n",
        "...\n",
        "\n",
        "a diferença para a inferencia é que no treinamento durante a geração da saida, \n",
        "o decoder é realimentado com a saida esperada e não a saida gerada.\n",
        "\n",
        "\"\"\"\n",
        "print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "uAXk1O0SNRqC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential, Model\n",
        "\n",
        "\n",
        "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
        "\n",
        "#Encoder\n",
        "encoder = LSTM(latent_dim, return_state=True)\n",
        "\n",
        "#pega saida do encoder\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "encoder_states = [state_h, state_c] #mantem apenas os estados internos\n",
        "\n",
        "\n",
        "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
        "\n",
        "#Decoder\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "\n",
        "#pega saída do decoder\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
        "                                     initial_state=encoder_states) #inicializa os estados interno do decoder com os do encoder\n",
        "\n",
        "#camada Dense com softmax do tamanho do vocabulario para definir qual o caracter de saída\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "#O modelo para o treino tem como entrada o input a ser codificado e o input do Force Teaching do decoder\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RcWvkVARNRqE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## word-level model with integer sequences\n",
        "# anteriormente deve-se separar por palavras e nao caracteres\n",
        "\n",
        "#from keras.models import Sequential, Model\n",
        "\n",
        "## Define an input sequence and process it.\n",
        "#encoder_inputs = Input(shape=(None,))\n",
        "#x = Embedding(num_encoder_tokens, latent_dim)(encoder_inputs)\n",
        "#x, state_h, state_c = LSTM(latent_dim,\n",
        "#                           return_state=True)(x)\n",
        "#encoder_states = [state_h, state_c]\n",
        "\n",
        "## Set up the decoder, using `encoder_states` as initial state.\n",
        "#decoder_inputs = Input(shape=(None,))\n",
        "#x = Embedding(num_decoder_tokens, latent_dim)(decoder_inputs)\n",
        "#x = LSTM(latent_dim, return_sequences=True)(x, initial_state=encoder_states)\n",
        "#decoder_outputs = Dense(num_decoder_tokens, activation='softmax')(x)\n",
        "\n",
        "## Define the model that will turn\n",
        "## `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "#model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "03ifnoZsNRqG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "88hf9EohNRqI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Uma das formas mais elegantes de salvar os pesos e o modelo é via callbacks:"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "1FOt6XfuNRqI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# checkpoint\n",
        "#filepath=\"weights-improvement-{epoch:02d}-{val_acc:.2f}.h5\"\n",
        "filepath = \"seq2seq_weights.h5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "t1PoZQpDNRqL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "epochs = 1\n",
        "\n",
        "# Executa o treino\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs, shuffle=False, #Sem shuffle para não misturar frases de tamanhos diferentes\n",
        "          validation_split=0.1, verbose = 1, callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BmfaPYdrNRqO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#uma forma mais simples de salvar os pesos é usando a função save_weights, que salvará o ultimo estado da rede\n",
        "#model.save_weights('seq2seq_weights.h5')\n",
        "#ou salva o modelo todo utilizando save\n",
        "model.save('s2s_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y_BVfZzuNRqQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Pesos do treino já executado\n",
        "model.load_weights('seq2seq_weights.h5')\n",
        "#carrega tudo: peso e modelo\n",
        "#mode.load('s2s_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-p1ddiYYNRqS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A principal diferença entre o modelo de testes e o de treinamento é que no treino utilizamos o force teaching e no teste iremos usar a real saída do decoder como input"
      ]
    },
    {
      "metadata": {
        "id": "GjVXTEKvNRqT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Modelo do Encoder\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "#Inputs para o decoder\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "#Reutiliza a LSTM treinada anteriormente com entradas novas\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "    decoder_inputs, initial_state=decoder_states_inputs)\n",
        "\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "#Agora nosso modelo possui como entrada os estados que irão ser gerados pelo encoder\n",
        "#e o output do decoder gerado pelos tempos anteriores\n",
        "\n",
        "#Como saída temos também os estados para garantir que no próximo tempo o Decoder continue de onde parou\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YZbv9AA-NRqV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Cria dicionário de tokens inverso para decodificar a saída\n",
        "reverse_input_char_index = dict(\n",
        "    (i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict(\n",
        "    (i, char) for char, i in target_token_index.items())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d23NBXW_NRqX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A função decode_sequence abaixo irá gerar a saída do decoder, ela é diferente da rede de treino pois agora a cada passo realimentamos com a ultima saída gerada (e nao a ultima saída esperada)"
      ]
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "wa-bSRVLNRqY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Aplica as entradas no encoder para pegar seus estados (sua memória)\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Gera um vetor de entrada nulo para o decoder\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    # coloca o \\t na primeira posição do vetor indicando para o decoder que deve iniciar a gerar texto\n",
        "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
        "\n",
        "    # Faz o loop gerando saídas do Decoder\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition: # gere novos tokens enquanto nao encontrar uma condição de parada \n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value)\n",
        "        \n",
        "        # Pega o token de saída\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        # Traduz para texto\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        # Adiciona à string de resposta\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Se a saída for \\n ou tiver atingido o tamanho máximo, para o loop\n",
        "        if (sampled_char == '\\n' or\n",
        "               len(decoded_sentence) > max_decoder_seq_length):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Recria o input do decoder com a última saída\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HO0CrNGWNRqb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#teste\n",
        "\n",
        "for seq_index in range(10000,10010):\n",
        "    # Take one sequence (part of the training set)\n",
        "    # for trying out decoding.\n",
        "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', input_texts[seq_index])\n",
        "    print('Decoded sentence:', decoded_sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RF1t-JvkhsJY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Abaixo temos a função de teste onde dado um texto em ingês ele irá retornar a tradução em português"
      ]
    },
    {
      "metadata": {
        "id": "GOBKFOTYNRqe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def teste(teste):\n",
        "    texto_seq = np.zeros((1,max_encoder_seq_length,num_encoder_tokens))\n",
        "\n",
        "    for t, char in enumerate(teste):\n",
        "        texto_seq[0, t, input_token_index[char]] = 1.\n",
        "    \n",
        "    decoded_sentence = decode_sequence(texto_seq)\n",
        "    print('-')\n",
        "    print('Input:', teste)\n",
        "    print('Output:', decoded_sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0IZTV7jsNRqh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(max_encoder_seq_length)\n",
        "print(num_encoder_tokens)\n",
        "teste('Have fun and be')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RQ-WUKD7NRqi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}